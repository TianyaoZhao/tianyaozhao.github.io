## 传统分类网络

## 传统2D目标检测网络

## Transformer

[transformer面试题的简单回答 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363466672)

1. 介绍自注意力机制和数学公式

   [Transformer学习笔记二：Self-Attention（自注意力机制） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/455399791)

   Self-attention的意思是，我们给Attention的输入都来自同一个序列，其计算方式如下：

   ![img](.assets/v2-980e1c0abb79f7d16785a59d55618991_r.jpg)

   对于每个token，产生三个向量，query，key，value

   query：类比询问，某个token问“其余的token”和我有多大程度的相关啊？

   key：类比索引，某个token说“我把每个询问的内容的回答都压缩了下，装在了我的key里”

   value：类比回答，某个token说“我把我自身涵盖的信息又抽取了一层封装在我的value中”

   **产生query、key、value**

   

   ![img](.assets/v2-ab15fd18a31f3a63ef100688e9a5747f_1440w.webp)

   ![image-20240430073432496](.assets/image-20240430073432496.png)

   为什么要除以dk？QK的数值可能会比较大，交叉熵比较小，不方便计算

   ![img](.assets/v2-a5856289c63896fbb4dd9e7151bbde67_1440w.webp)

2. VIT

   将图像转换成一系列的token，将图像分割开，每一个作为一个token，显然比较吃计算

   因此选择将图像分割成小的patchs，将每一个patchs enbedding成一个token

   

   ![](.assets/image-20231206172342744.png)

   ![](.assets/image-20231206172427580.png)

   1. patch embeding

      ![image-20231206172814623](.assets/image-20231206172814623.png)

   ![image-20231206173448490](.assets/image-20231206173448490.png)

   

   2. class embedding and positional embedding

      **位置编码和class编码**都是可学习的编码

      ![image-20231206173616674](.assets/image-20231206173616674.png)

3. 介绍Transformer的QKV

   以图中的token a2为例：

   - 它产生一个query，每个query都去和别的token的key做“**某种方式**”的计算，得到的结果我们称为attention score（即为图中的$$\alpha $$）。则一共得到四个attention score。（attention score又可以被称为attention weight）。
   - 将这四个score分别乘上每个token的value，我们会得到四个抽取信息完毕的向量。
   - 将这四个向量相加，就是最终a2过attention模型后所产生的结果b2

4. 介绍Layer Normalization

   nlp用ln，图像用bn

5. Transformer的训练和部署技巧

6. 介绍Transformer的位置编码

   [Transformer学习笔记一：Positional Encoding（位置编码） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/454482273)

   因此，我们需要这样一种位置表示方式，满足于：
   （1）它能用来表示一个token在序列中的绝对位置
   （2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致
   （3）可以用来表示模型在训练过程中从来没有看到过的句子长度。

7. multi-head attention

   ![image-20231206171636403](.assets/image-20231206171636403-171443494901417.png)

   将qkv拆分为h份，然后在每一层上计算self-attention，最后再把每个token产生的b1~bh拼接起来，类似于不同的channel数

   

8. 介绍Transformer的Encoder模块

   ![img](.assets/v2-f6380627207ff4d1e72addfafeaff0bb_r.jpg)

9. 介绍Transformer的Decoder模块

10. Transformer和Mamba（SSM）的区别

11. Transformer的残差结构和意义

12. 为什么Transformer适合多模态任务

13. Transformer的并行化体现在哪些方面

    Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行

14. 为什么Transformer一般使用LayerNorm

15. Transformer为什么使用多头注意力机制

    多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。

16. Transformer训练的Dropout是如何设定的

## BEV感知

1. 什么是BEV感知？

    **2D感知任务**是从单张图像/时间序列中检测或者分割目标，但是对于**自动驾驶**任务来说，最终需要目标在**自车坐标系**下的相关信息，因此2D的感知结果，需要映射到3D空间。即便是2D视觉感知自身发展比较成熟，但是直接从场景中获取目标的3D信息是一件比较困难的事情。对于**单目相机**，由于相机成像几何的局限性，深度估计是一个病态的问题。更好的方式是通过**双目相机**来进行深度估计，但是视野范围小，作用有限。因此有了**多目感知**，通过安装在车身上的六个摄像头来进行**3D感知任务**，也是在这个系统中提出了**BEV感知**的概念。也就是**将来自多个摄像头的图像，从透视图变换到鸟瞰图（Bird‘s Eye View）**，BEV视图包含了对自动驾驶任务重要的**位置信息**，对于**目标的高度信息**隐藏在了BEV特征图中，因此BEV感知的结果，可以直接用于下游的各种任务，对于BEV感知任务，最重要的我认为就是**从2D空间到3D空间的变换**，同样为了提高准确率，也有**点云激光雷达、毫米波雷达以及图像**的**多模态融合**方案来进行BEV感知任务

2. BEV感知任务采用的数据集和指标？

    nuScenes数据集

    **IOU**：IoU采用BEV视图下，2D检测框的中心点的距离来进行匹配，而不是采用3D框的交并比来实现，这样可以提高小目标的匹配成功率。

    **TP（True Postive）**：真正例，IOU>0.5的检测框数目

    **FP（False Postive）**：假正例，IOU<0.5的检测框数目

    **TN（True Negative）**：真负例，没有检测框

    **FN（False Negative）**：假负例，没有检测到GT框的数目

    **Precision**： $查准率=\frac{TP}{TP+FP}$

    **Recall**：$查全率 = \frac{TP}{TP + FN}$

    **AP（Average Precision）**：Precision和Recall在某些情况下是矛盾的，查得准，但是不一定全，查的全，但是不一定查的准，我们绘制PR曲线，计算PR曲线的面积（平均值）

    ![img](./.assets/v2-8e5dea717aa368da9e66d91f4fbe053c_1440w.webp)

    **mAP**：**针对每个类别**求AP，多个类别AP求平均值，即为mAP

    **NDS（nuScenes Detection Score）**：nuScenes数据集的一个指标，在mAP的基础上，还预测物体框的位置，大小，朝向，速度以及其他属性。可以更全面的评价3D目标检测算法的优劣

3. BEV感知中的**BaseLine**工作？

    1. **从2D空间到3D空间**

        1. LSS（Lift，Splat，Shoot）

            **a.**多张视图通过共享的CNN主干网络提取特征，对每一个特征图进行**视图转换**，核心步骤是**估计特征图每个像素处的深度分布**，这里没有预测具体的深度值，而是设置**D个离散的深度值**，网络在**每个像素处预测D个概率值**$\alpha_1,\alpha_2,\ldots,\alpha_D$，以及该**像素处的通道特征c（维度是C）**，用$\alpha$对$c$加权就得到了对应每个深度的上下文特征，这个过程叫做**Lift**

            ![img](./.assets/v2-7d308520ddf235479c55a4e62d768554_1440w.webp)

            **b.**假设特征图的大小为$H×W$，每个像素有$D$个可能的深度值。通过相机的内外参数，每个像素可以转换为3D空间中的D个点，因此每个特征图可以转换为$H×W×D$个点，有$N$个相机，一共有$N×H×W×D$个3D点，这些三维点可以利用**PointPillar**的方式进行投影，一个Pillar对应一个BEV网格，对落到网格中的点进行Pooling操作，得到$C$维的特征，所以BEV特征图的维度是$X×Y×C$，$X、Y$是BEV网格横向和纵向的个数，这个过程叫**Splat**。

            **c.**Shoot就是用得到的BEV特征图做下游任务（检测、分割）

        2. BEVDet

            ![img](./.assets/v2-380b98a93459385e46a591ef8082f212_1440w.webp)

            **a.**BEVDet以LSS的视图转换为基础，**但是在BEV视图中采用了额外的特征提取网络来进行进一步的特征提取**，对进一步提取生成的BEV特征进行检测。

            **b.**其特征提取的主干网络可以采用ResNet、也可以采用Swin—Transformer，最后采用**CenterPoint**的Head结构来进行3D检测任务，同时改进了**非极大值抑制策略**

            ![image-20240425170346498](./.assets/image-20240425170346498.png)

        3. BEVDet4D

            ![img](./.assets/v2-c48244d5137b76d5264d0ebfb43a788c_1440w.webp)

            **a.**BEVDet在**时序融合上的扩展**，对于单帧图像的处理采用了BEVDet同样的方法，将图像转换到BEV视图上，并进行特征提取。时序处理的方式非常简单：**将T-1时刻和T时刻的BEV特征进行空间对齐**

            **b.**对于自车的运动，通过平移和旋转，将T-1时刻的坐标对齐到T时刻，因此可以通过简单的叠加操作来完成时序的融合

            ![image-20240425171440735](./.assets/image-20240425171440735.png)

        4. BEVDepth

            ![image-20240425171951570](./.assets/image-20240425171951570.png)

            为了改善LSS方式的深度估计质量较差的问题，**BEVDepth在训练过程中加入额外的深度信息监督**。具体来说，**激光雷达的点云被投影到图像上，为深度提供显式的监督信息**

        5. BEVFusion

            ![img](./.assets/v2-4f71701cf5679f0f01bd2f09ae66cd4e_1440w.webp)

            **a.**图像与点云激光雷达的**并行**融合方案

            **b.**图像之路仍然采用LSS式的特征提取

            **c.**点云之路采用点云BEV特征提取，然后经过和图像的融合模块得到最终的BEV特征图

            **d.**在每一条分支上加上Head，以此来监督每一条分支，使结果更准确

    2. **从3D空间到2D空间**

        1. DETR3D

            ![img](./.assets/v2-9a381cc992bab9142d9e7f43d4908160_1440w.webp)

            **a.**采用**稀疏的目标级别的query，每一个query对用一个目标的3D中心点**，通过相机成像几何，**将3D点投影到2D平面中，从图像取特征，填回到BEV特征中**，通过这个BEV特征进行检测，来更新query，反复迭代

            ![img](./.assets/v2-8eb12c3dcfd113f34219f77c9eb45d11_1440w.webp)

        2. PETR

            **a.** 和DETR一样，不同的是，PETR的query并没有和DETR一样**通过3D到2D的映射与图像特征建立联系**，而是通过**PostionEnbeding**，**在2D图像特征上编码3D空间信息**

            **b.**缺点是显而易见的，必须要计算query和图像特征之间的**全局注意力，因为query无法直接与图像的局部区域建立联系**，计算量非常大。

        3. BEVFormer

            **多尺度可变形注意力**

            ![img](./.assets/v2-5ef798d620bbb9d1ec9b9d9f22c3739c_1440w.webp)

            规而言 Attention Map = Query 和 Key 做内积运算，将 Attention Map 再和 Value 做加权；但是由于这种方式计算量开销会比较大，所以在 Deformable DETR 中**用局部注意力机制代替了全局注意力机制**，只对几个采样点进行采样，**而采样点的位置相对于参考点的偏移量和每个采样点在加权时的比重均是靠 Query 经过 Linear 层学习得到的**。

            

            ![img](./.assets/v2-c5ffa7fd0d0f4440c3e1e6c0ef422f8e_1440w.webp)

            **a.**BEVFormer采用**空间交叉注意力**来进行**图像到BEV视图**的转换，其中**query来自BEV视图，而key和value来自于图像视图**，query被定义为**可学习的BEV网格特征**，大小为$H×W×C$，$HW$是BEV网格个数，C是特征维度

            **交叉注意力中query和key/value的来源不同，它经常被用于不同域（Domain）之间的数据转换。**

            **b.**为了提取3D特征，BEV query的每个网格都被扩展为高度不同的 $N_{ref}$个三维点，从而通过高度信息，更好的从2D图像上取特征

            **c.**时序融合上采用**时序自注意力机制**，其中**query来自当前帧的BEV特征图，key和value来自历史BEV特征图**

        4. BEVFormerV2

            该方法没有用额外的激光雷达点云来提供深度信息监督，而是**给主干网络增加一个额外的3D物体检测任务（**下图中绿框的部分）。这是一个辅助任务，可以帮助主干网络更好的提取3D信息

            ![img](./.assets/v2-cde6b0680f30839998c1ea19ee2644a9_1440w.webp)

    3. 什么是BEV Occupancy

        直接在BEV空间中完成动态障碍物的3D检测这是BEV感知的任务，直接在3D空间中感知占用关系这是occupancy
    
        BEV模型和Occ有很多相似之处，bev特征生成范式和occ几乎完全相同，时序方法也适用。
    
        **数据生成和优化**占据了occ任务的主要部分，occ的真值很难人工标注，需要借助3dbbx间接生成。这就需要优化真值
    
        相比于之前的3D感知算法而言，基于栅格网络的感知算法更类似于**基于3D空间的语义分割任务**，算法将当前的感知空间划分成**体素**进行表示，然后对空间中的每个体素预测其**是否被占用**以及**被占用的体素所属的类别信息**
    
        1. 更准确的描述物体的外部轮廓
        2. 可以预测数据集以外的目标类别，general object
    
    4. 
    
    
    
    ## OpenMMlab
    
    ![img](.assets/format,png.png)
    
    1. 介绍一下openmmlab的框架
    
       openmmlab为计算机视觉方向创建了一个**统一且开源**的代码库。
    
    2. openmmlab的特点
    
       1. 模块化组合设计：将网络框架拆分为不同的组件。将数据集的构建、模型的搭建、训练过程封装为模块，在一个统一的架构上供用户自定义创建网络架构
       2. 高性能：基于底层库mmcv，几乎所有的操作都在GPU上运行
       3. SOTA方法：开源框架不断集成计算机视觉领域的先进算法
    
    3. mmcv核心组件分析
    
       1. config
    
          通过Config.fromfile遍历config.py文件
    
          遍历所有的base文件，然后再合并自己定义的配置文件，最终得到配置文件字典
    
       2. registry
    
          提供全局的类注册器功能，Registry类维护一个全局的key-value对，比如backbone、head、neck等，用户可以通过字符串的方式实例化任何想要的模块
    
       3. hook
    
          通过注册hook，可以拦截和修改某些中间变量的值。随意插入任何函数来捕获中间过程，本质上是pytorch内部的回调函数
    
          mmcv中，当runner运行到预定义的点位时，就会调用hook中对应的方法，例如保存ckpt，学习率调度，参数更新，日志打印等等
    
          ![img](.assets/v2-9f97b0517800009600c70d60db0a1229_1440w.webp)
    
       4. runner
    
          runnner的使用过程可以分为4个步骤
    
          1. runner对象初始化
          2. 注册各种hook到runner中
          3. 调用runner的resume或load_ckpt对权重进行加载
          4. 执行给定的工作流



## C++

1. 什么是虚函数、什么是纯虚函数

   [C++ 虚函数、纯虚函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37331092)

   多态是面向对象编程语言的一大特点，而虚函数是实现多态的机制。**核心理念是**通过基类访问派生类定义的函数，**使用一个基类的指针或引用，进而调用子类复写的个性化虚函数，是C++实现多态的一个场景**

   虚函数：在类成员方法中声明 `virtual void func()`

   纯虚函数：在类成员方法中生命`virtual void func()=0`

   对于虚函数，子类也可以定义也可以不重新定义基类的虚函数

   对于纯虚函数，子类**必须重新定义**基类的纯虚函数

2. 

## PYTHON

1. 什么是装饰器

   [一文搞懂什么是Python装饰器？Python装饰器怎么用？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/107917604)

   装饰器，顾名思义，就是**增强**函数或者类功能的一个函数

   **定义装饰器：**

   ```python
   def decorator(func):
       def wrapper(*args,**kargs): 
           # 可以自定义传入的参数        
           print(func.__name__)
           # 返回传入的方法名参数的调用
           return func(*args,**kargs) 
       # 返回内层函数函数名
       return wrapper
   
   ```

   

2. 

